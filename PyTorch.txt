PyTorch offers APIs to control the number of threads used:

import torch

# Set number of intra-op threads (parallelism within an operation)
torch.set_num_threads(8)  # Adjust as needed

# Set number of inter-op threads (parallelism across independent operations)
torch.set_num_interop_threads(2)  # Adjust as needed

Set these before running your Python script (bash):
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
python yourscript.py


You can check how many threads are being used:
print(torch.get_num_threads())
print(torch.get_num_interop_threads())





And for FFT (with MKL):
import mkl
print(mkl.get_max_threads())
